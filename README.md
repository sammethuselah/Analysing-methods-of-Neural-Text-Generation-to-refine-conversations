# Analysing-methods-of-Neural-Text-Generation-to-refine-conversations
A good conversation requires a combination of simplicity and detail, staying on topic and shifting it, and asking and answering questions. Although human evaluations of overall quality are routinely used to evaluate dialogue agents, the relationship between overall quality and these individual factors are not well-studied. In this paper, we look at conditional training and weighted decoding as two adjustable neural text generation strategies for controlling four dialogue attributes: repetition, specificity, response-relatedness, and question-asking. We use the PersonaChat task to undertake a large-scale human evaluation to see how these control factors affect multi-turn interactive talks. We examine their relationship to high-level elements of discourse in depth, demonstrating that manipulating combinations of these variables improves human quality assessments significantly.
